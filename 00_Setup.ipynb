{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d76552-9f14-4c34-96dd-6be7b2f2df30",
   "metadata": {},
   "source": [
    "# Setup notebook: preload model weights to save time later\n",
    "\n",
    "Before we dive into our main presentation, we'll do a tiny bit of setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86cc77c-5be6-46d9-b1ad-45479fa0f504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21325cf-fc49-4b69-8115-0b2aa144fc4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "We'll create the pipeline that will be used later, to trigger full download of the weights and verify we can load them.\n",
    "\n",
    "> Note the `cache_dir` kwarg to use the fast NVMe storage on Anyscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f937b87-a71f-4630-9955-def529143a58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "p  = pipeline(model=\"stabilityai/stablelm-tuned-alpha-7b\", task='text-generation', \n",
    "              model_kwargs={'device_map':'auto', 'torch_dtype' : torch.float16, 'cache_dir': '/mnt/local_storage/'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d66a2-80f7-4635-9680-c75f8f652a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd41ea7-d6a0-4542-9fd9-9f4f29a487e7",
   "metadata": {},
   "source": [
    "Note that our model is loaded into GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1511c-258b-4d46-8438-d0c45deea218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc97733-5aa8-41b0-9b2c-276b65a6589a",
   "metadata": {},
   "source": [
    "In production situations, this memory should be freed when the process exits.\n",
    "\n",
    "However, in a notebook (or other long-running dev process environment), it can be useful to purge unneeded data directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca91486-64dd-4e9a-8906-5bec952f0185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a33639-1cf3-4887-8823-2e0dc6c847ab",
   "metadata": {},
   "source": [
    "> ðŸ¤— Accelerate is a library that enables the same PyTorch code to be run across any distributed configuration by adding just four lines of code! In short, training and inference at scale made simple, efficient and adaptable.\n",
    "\n",
    "Here, we can use it directly to clear GPU space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b7d5e1d-88fb-4653-a7cf-177c4332df47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "accelerator.free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc38be-3083-473e-90f2-11e661398e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
