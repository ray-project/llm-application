{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9fdc8f2-9873-43ad-b3a0-cb26cfa54de1",
   "metadata": {},
   "source": [
    "# Build and deploy LLM-based applications"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b0c0fb4-cefa-4766-a498-46860014cac3",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Example: document-based Q&A app featuring LangChain, Huggingface, FAISS, and a StabilityAI StableLM model\n",
    "* Intro to Ray\n",
    "* Why Ray for LLM Production?\n",
    "* Intro to Ray Serve\n",
    "    * Deploying a multilingual chat app with Ray Serve\n",
    "* Porting document-based Q&A to Ray Serve for production\n",
    "    * Deploying with Anyscale services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70854f85-c97f-4d7c-a96f-fe5820d64f85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "FAISS_INDEX_PATH = \"faiss_index_local\"\n",
    "\n",
    "topics = ['The Eras Tour', '2023 XFL season']\n",
    "loaders = [WikipediaLoader(query=topic, load_max_docs=20) for topic in topics]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20, length_function=len,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f9a33-4716-4094-b0c5-b13f2382cd27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "docs = add(*[loader.load() for loader in loaders])\n",
    "print([d.metadata['title'] for d in docs])\n",
    "\n",
    "chunks = text_splitter.create_documents([doc.page_content for doc in docs], metadatas=[doc.metadata for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a228d-4722-4c0d-8b97-b8bb51a9f59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalHuggingFaceEmbeddings(Embeddings):\n",
    "    def __init__(self, model_id):\n",
    "        self.model = SentenceTransformer(model_id)\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        embedding = self.model.encode(text)\n",
    "        return list(map(float, embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def90b66-216e-4f5b-b68d-c022c1b99a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = LocalHuggingFaceEmbeddings(\"multi-qa-mpnet-base-dot-v1\")\n",
    "db = FAISS.from_documents(chunks, embeddings)\n",
    "db.save_local(FAISS_INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625306aa-195c-455b-b935-7081c86c33ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional, Any\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import pipeline as hf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bde69c-5642-4611-ab98-76cca879a137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StableLMPipeline(HuggingFacePipeline): \n",
    "    # Class is temporary, we are working with the authors of LangChain to make these unnecessary.\n",
    "    \n",
    "    def _call(self, prompt: str, stop: Optional[list[str]] = None) -> str:\n",
    "        response = self.pipeline(prompt, temperature=0.1, max_new_tokens=256, do_sample=True)\n",
    "        print(f\"Response is: {response}\")\n",
    "        text = response[0][\"generated_text\"][len(prompt) :]\n",
    "        return text\n",
    "\n",
    "    @classmethod\n",
    "    def from_model_id(cls, model_id: str, task: str, device: Optional[str] = None, model_kwargs: Optional[dict] = None, **kwargs: Any,):\n",
    "        pipeline = hf_pipeline(model=model_id, task=task, device=device, model_kwargs=model_kwargs, )\n",
    "        return cls(pipeline=pipeline, model_id=model_id, model_kwargs=model_kwargs, **kwargs, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af642edf-f6e4-47d9-ad62-964298cc7414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- You are a helpful, polite, fact-based agent for answering questions. \n",
    "- Your answers include enough detail for someone to follow through on your suggestions. \n",
    "<|USER|>\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "Please answer the following question using the context provided. \n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "=========\n",
    "QUESTION: {question} \n",
    "ANSWER: <|ASSISTANT|>\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd2ba8-0d80-493e-8df4-b32f315a39a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QALocal:\n",
    "    def __init__(self):\n",
    "        self.embeddings = LocalHuggingFaceEmbeddings(\"multi-qa-mpnet-base-dot-v1\")\n",
    "        self.db = FAISS.load_local(FAISS_INDEX_PATH, self.embeddings)\n",
    "        self.llm = StableLMPipeline.from_model_id(\n",
    "            model_id=\"stabilityai/stablelm-tuned-alpha-7b\",\n",
    "            task=\"text-generation\",\n",
    "            model_kwargs={\"torch_dtype\": torch.float16, \"device_map\": \"auto\", 'cache_dir':'/mnt/local_storage'}\n",
    "        )\n",
    "        self.chain = load_qa_chain(llm=self.llm, chain_type=\"stuff\", prompt=PROMPT)\n",
    "\n",
    "    def qa(self, query):\n",
    "        search_results = self.db.similarity_search(query)\n",
    "        print(f\"Results from db are: {search_results}\")\n",
    "        result = self.chain({\"input_documents\": search_results, \"question\": query})\n",
    "        print(f\"Result is: {result}\")\n",
    "        return result[\"output_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1628de-7501-4a00-a0e0-7fe82b3d8619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_qa = QALocal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a49947-d2cd-40b3-8595-acfab33c6dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_qa.qa(\"How many people live in San Francisco?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604df4f-1928-408c-a820-ee134ef72078",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_qa.qa(\"When did Taylor Swift's Eras tour start?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e7d46e-d84f-4e68-9da7-344621805af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_qa.qa(\"Can you tell me about the XFL 2023 season?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc66fa8-5ea1-46dd-a6b7-cb4524af6d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del(local_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462df2c4-14c9-4a14-ae7a-8f10ccdbb51b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "accelerator.free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e7f52-bb8b-49e8-ac4f-1d05d4f84817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
